#!/usr/bin/env python
# -*- coding: utf-8 -*-
 
'''
 Deep Belief Nets (DBN)
'''
import sys
sys.path.append('/home/pesuchin/Desktop/libsvm-3.18/python/')
from svm import *
from svmutil import *
import numpy
 
numpy.seterr(all='ignore')
 
def sigmoid(x):
    return 1. / (1 + numpy.exp(-x))
 
class DBN(object):
    def __init__(self, v=None, label=None,\
                 n_ins=2, hidden_layer_sizes=[3, 3], n_outs=2,\
                 numpy_rng=None):
        
        self.x = v
        self.y = label
 
        self.sigmoid_layers = []
        self.rbm_layers = []
        self.n_layers = len(hidden_layer_sizes)  # = len(self.rbm_layers)

        if numpy_rng is None:
            numpy_rng = numpy.random.RandomState(1234)
 
        
        assert self.n_layers > 0
 
        # construct multi-layer
        for i in xrange(self.n_layers):
            # layer_size
            if i == 0:
                v_size = n_ins
            else:
                v_size = hidden_layer_sizes[i - 1]

            # layer_v
            if i == 0:
                layer_v = self.x
            else:
                layer_v = self.sigmoid_layers[-1].sample_h_given_v()

            # construct sigmoid_layer
            sigmoid_layer = HiddenLayer(v=layer_v,
                                        n_in=v_size,
                                        n_out=hidden_layer_sizes[i],
                                        numpy_rng=numpy_rng,
                                        activation=sigmoid)
            self.sigmoid_layers.append(sigmoid_layer)
 
            # construct rbm_layer
            rbm_layer = RBM(v=layer_v,
                            n_visible=v_size,
                            n_hidden=hidden_layer_sizes[i],
                            W=sigmoid_layer.W,     # W, b are shared
                            hbias=sigmoid_layer.b)
            self.rbm_layers.append(rbm_layer)
        
        # layer for output using SVM
        data = self.sigmoid_layers[-1].sample_h_given_v().tolist()
        self.SVM_layer = svm_problem(self.y,data)
 
    def pretrain(self, lr=0.1, k=1, epochs=100):
        # pre-train layer-wise
        for i in xrange(self.n_layers):
            if i == 0:
                layer_v = self.x
            else:
                layer_v = self.sigmoid_layers[i-1].sample_h_given_v(layer_v)
            rbm = self.rbm_layers[i]
            
            for epoch in xrange(epochs):
                rbm.contrastive_divergence(lr=lr, k=k, v=layer_v)
 
    def finetune(self, lr=0.1, epochs=100,param='-s 0 -t 0'):
        layer_v = self.sigmoid_layers[-1].sample_h_given_v()
 
        # train SVM
        self.param = svm_parameter(param) # 学習方法の設定
        self.m = svm_train(self.SVM_layer, self.param)
    
 
    def predict(self, x,test_label):
        layer_v = x
        
        for i in xrange(self.n_layers):
            sigmoid_layer=self.sigmoid_layers[i]
            layer_v = sigmoid_layer.output(v=layer_v)
	auc.append(self.m)
        #result = svm_predict(test_label,x,self.m)
        return result
 
class HiddenLayer(object):
    def __init__(self, v, n_in, n_out,\
                 W=None, b=None, numpy_rng=None, activation=numpy.tanh):
        
        if numpy_rng is None:
            numpy_rng = numpy.random.RandomState(1234)
 
        if W is None:
            a = 1. / n_in
            initial_W = numpy.array(numpy_rng.uniform(  # initialize W uniformly
                low=-a,
                high=a,
                size=(n_in, n_out)))
 
            W = initial_W
 
        if b is None:
            b = numpy.zeros(n_out)  # initialize bias 0
 
 
        self.numpy_rng = numpy_rng
        self.v = v
        self.W = W
        self.b = b
 
        self.activation = activation
 
    def output(self, v=None):
        if v is not None:
            self.v = v
        linear_output = numpy.dot(self.v, self.W) + self.b
 
        return (linear_output if self.activation is None
                else self.activation(linear_output))
 
    def sample_h_given_v(self, v=None):
        if v is not None:
            self.v = v
 
        v_mean = self.output()
        
        h_sample = self.numpy_rng.binomial(size=v_mean.shape,
                                           n=1,
                                           p=v_mean)
        return h_sample
 
 
 
class RBM(object):
    def __init__(self, v=None, n_visible=2, n_hidden=3, \
        W=None, hbias=None, vbias=None, numpy_rng=None):
        
        self.n_visible = n_visible  # num of units in visible (v) layer
        self.n_hidden = n_hidden    # num of units in hidden layer
 
        if numpy_rng is None:
            numpy_rng = numpy.random.RandomState(1234)
 
        if W is None:
            a = 1. / n_visible
            initial_W = numpy.array(numpy_rng.uniform(  # initialize W uniformly
                low=-a,
                high=a,
                size=(n_visible, n_hidden)))
 
            W = initial_W
 
        if hbias is None:
            hbias = numpy.zeros(n_hidden)  # initialize h bias 0
 
        if vbias is None:
            vbias = numpy.zeros(n_visible)  # initialize v bias 0
 
 
        self.numpy_rng = numpy_rng
        self.v = v
        self.W = W
        self.hbias = hbias
        self.vbias = vbias

        # self.params = [self.W, self.hbias, self.vbias]
 
 
    def contrastive_divergence(self, lr=0.1, k=1, v=None):
        if v is not None:
            self.v = numpy.array(v)
        self.v = numpy.array(v)
        ''' CD-k '''
        ph_mean, ph_sample = self.sample_h_given_v(self.v)
 
        chain_start = ph_sample
        for step in xrange(k):
            if step == 0:
                nv_means, nv_samples,\
                nh_means, nh_samples = self.gibbs_hvh(chain_start)
            else:
                nv_means, nv_samples,\
                nh_means, nh_samples = self.gibbs_hvh(nh_samples)
 
        #chain_end = nv_samples
        
        #update
        self.W += lr * (numpy.dot(self.v.T, ph_sample)
                        - numpy.dot(nv_samples.T, nh_means))
        self.vbias += lr * numpy.mean(self.v - nv_samples, axis=0)
        self.hbias += lr * numpy.mean(ph_sample - nh_means, axis=0)
 
        cost = self.get_reconstruction_cross_entropy()
        return cost
 
    
    def sample_h_given_v(self, v0_sample):
        h1_mean = self.propup(v0_sample)
        h1_sample = self.numpy_rng.binomial(size=h1_mean.shape,   # discrete: binomial
                                       n=1,
                                       p=h1_mean)
        return [h1_mean, h1_sample]
 
 
    def sample_v_given_h(self, h0_sample):
        v1_mean = self.propdown(h0_sample)
        v1_sample = self.numpy_rng.binomial(size=v1_mean.shape,   # discrete: binomial
                                            n=1,
                                            p=v1_mean)
        return [v1_mean, v1_sample]
        
    #conditional probability for updating
    def propup(self, v):
        pre_sigmoid_activation = numpy.dot(v, self.W) + self.hbias
        return sigmoid(pre_sigmoid_activation)
    
    def propdown(self, h):
        pre_sigmoid_activation = numpy.dot(h, self.W.T) + self.vbias
        return sigmoid(pre_sigmoid_activation)
 
    def gibbs_hvh(self, h0_sample):
        v1_mean, v1_sample = self.sample_v_given_h(h0_sample)
        h1_mean, h1_sample = self.sample_h_given_v(v1_sample)
 
        return [v1_mean, v1_sample,
                h1_mean, h1_sample]
 
 
    def get_reconstruction_cross_entropy(self):
        pre_sigmoid_activation_h = numpy.dot(self.v, self.W) + self.hbias
        sigmoid_activation_h = sigmoid(pre_sigmoid_activation_h)
        
        pre_sigmoid_activation_v = numpy.dot(sigmoid_activation_h, self.W.T) + self.vbias
        sigmoid_activation_v = sigmoid(pre_sigmoid_activation_v)
 
        cross_entropy =  - numpy.mean(
            numpy.sum(self.v * numpy.log(sigmoid_activation_v) +
            (1 - self.v) * numpy.log(1 - sigmoid_activation_v),
                      axis=1))
        
        return cross_entropy
 
    def reconstruct(self, v):
        h = sigmoid(numpy.dot(v, self.W) + self.hbias)
        reconstructed_v = sigmoid(numpy.dot(h, self.W.T) + self.vbias)
        return reconstructed_v
 
def readfile(filename):
  lines=[line for line in file(filename)]

  # First line is the column titles
  colnames=lines[0].strip().split('\t')[1:]
  rownames=[]
  data=[]
  for line in lines[1:]:
    p=line.strip().split('\t')
    # First column in each row is the rowname
    rownames.append(p[0])
    # The data for this row is the remainder of the row
    data.append([float(x) for x in p[1:]])
  return rownames,colnames,data

def main(feature=0):  
  if feature == 0:
    c_rownames,c_colnames,c_data = readfile('./../feature/cor_BOW.txt')
    m_rownames,m_colnames,m_data = readfile('./../feature/mal_BOW.txt')
  elif feature == 1:
    c_rownames,c_colnames,c_data = readfile('./../feature/cor_count.txt')
    m_rownames,m_colnames,m_data = readfile('./../feature/mal_count.txt')
  elif feature == 2:
    c_rownames,c_colnames,c_data = readfile('./../feature/cor_tf-idf.txt')
    m_rownames,m_colnames,m_data = readfile('./../feature/mal_tf-idf.txt')
  label = []
  #test_data = []
  #test_label = []
  data=[]
  print "Learning SVM started!"
  for i in c_data:
    label.append(1)
  for d in m_data:
    c_data.append(d)
    label.append(-1)
  f = open("./../result/AUC.csv",'a')
  save = csv.writer(f)
  kernel=[""]
  auc=[""]
  kernel.append("Linear")
  kernel.append("Polynomial")
  kernel.append("RBF")
  kernel.append("Sigmoid") 
  save.writerow(kernel)
  rng = numpy.random.RandomState(123)
  dbn = DBN(v=model,label=label,n_ins=len(data[1]),hidden_layer_sizes=[400,400,400,400,400],n_outs=2,numpy_rng=rng)
  dbn.pretrain(k=1)
  print "------------------------------------------------"
  print "Linear kernel"
  print "------------------------------------------------"
  prob = svm_problem(label,c_data) # データとラベルを合成
# 学習方法の設定
  dbn.finetune(param='-s 0 -t 0 -v %d' % cross)
  print dbn.predict(c_data,lavel)
  #result = svm_predict(test_label,test_data,m)
  print "------------------------------------------------"
  print "Polynomial kernel"
  print "------------------------------------------------"
# 学習方法の設定
  dbn.finetune(param='-s 0 -t 1 -v %d' % cross)
  print dbn.predict(c_data,lavel)
  #result = svm_predict(test_label,test_data,m)
  print "------------------------------------------------"
  print "Radial Basis Function(RBF) kernel"
  print "------------------------------------------------"
 # 学習方法の設定
  dbn.finetune(param='-s 0 -t 2 -v %d' % cross)
  print dbn.predict(c_data,lavel)
  #result = svm_predict(test_label,test_data,m)
  print "------------------------------------------------"
  print "Sigmoid kernel"
  print "------------------------------------------------"
# 学習方法の設定
  dbn.finetune(param='-s 0 -t 3 -v %d' % cross)
  print dbn.predict(c_data,lavel)
  #result = svm_predict(test_label,test_data,m)
  save.writerow(auc)
  f.close()

if __name__ == "__main__":
    main()
